import streamlit as st
import torch
import joblib
from sklearn.pipeline import Pipeline
from pymorphy3 import MorphAnalyzer
from nltk.corpus import stopwords
import string
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from streamlit_extras.let_it_rain import rain
from typing import Tuple
import numpy as np
import torch.nn.functional as F  # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –¥–æ–∂–¥—è
def eye():
    rain(
        emoji="üëÅ",
        font_size=100,
        falling_speed=6,
        animation_length="infinite",
    )

eye()


# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
st.image("savelyi.gif", width=1000)

# HTML –∏ CSS –¥–ª—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
html_code = """
<style>
@keyframes rainbow {
    0% { color: red; }
    14% { color: orange; }
    28% { color: yellow; }
    42% { color: green; }
    57% { color: blue; }
    71% { color: indigo; }
    85% { color: violet; }
    100% { color: red; }
}

.rainbow-text {
    font-size: 3em;
    font-weight: bold;
    text-align: center;
    animation: rainbow 5s infinite;
}
</style>

<div class="rainbow-text">–í–°–ï –û–¢–ó–´–í–´ –ü–û –°–í–û–ï–ú–£ –ü–û–ó–ò–¢–ò–í–ù–´–ï!</div>
"""

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ HTML –≤ Streamlit
st.markdown(html_code, unsafe_allow_html=True)


def padding(review_int: list, seq_len: int) -> np.array:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –ø–∞–¥–¥–∏–Ω–≥ –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º.
    :param review_int: –°–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å–ª–æ–≤.
    :param seq_len: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    :return: –ú–∞—Å—Å–∏–≤ —Å –ø–∞–¥–¥–∏–Ω–≥–æ–º.
    """
    features = np.zeros((len(review_int), seq_len), dtype=int)
    for i, review in enumerate(review_int):
        if len(review) <= seq_len:
            zeros = list(np.zeros(seq_len - len(review)))
            new = zeros + review
        else:
            new = review[:seq_len]
        features[i, :] = np.array(new)
    return features

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è Bahdanau
class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.attention = nn.Linear(hidden_size, 1)

    def forward(self, lstm_out, h_n):
        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
        context_vector = torch.sum(attention_weights * lstm_out, dim=1)
        return context_vector, attention_weights


# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è LSTM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è
class LSTMBahdanauAttention(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):
        super(LSTMBahdanauAttention, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)
        self.attn = BahdanauAttention(hidden_size)
        self.clf = nn.Sequential(
            nn.Linear(hidden_size, 128),
            nn.Dropout(0.2),
            nn.Tanh(),
            nn.Linear(128, num_classes),
            nn.Dropout(0.1)
        )

    def forward(self, x):
        embeddings = self.embedding(x)
        outputs, (h_n, _) = self.lstm(embeddings)
        context, att_weights = self.attn(outputs, h_n.squeeze(0))
        out = self.clf(context)
        return out, att_weights  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–≤–∞ –∑–Ω–∞—á–µ–Ω–∏—è


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
from typing import Tuple  # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç –∏–º–ø–æ—Ä—Ç
import numpy as np

@st.cache_resource
def load_models():
    bow_pipeline = None
    tfidf_pipeline = None
    bert_model = None
    lstm_model = None
    tokenizer = None
    word_to_int = None

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ pipeline –¥–ª—è Bag-of-Words + Logistic Regression
        bow_pipeline = joblib.load("bow_pipeline.joblib")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ Bag-of-Words: {e}")

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ pipeline –¥–ª—è TF-IDF + Logistic Regression
        tfidf_pipeline = joblib.load("tfidf_pipeline.joblib")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ TF-IDF: {e}")

    try:
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –º–æ–¥–µ–ª–∏ MyTinyBERT
        class MyTinyBERT(nn.Module):
            def __init__(self):
                super().__init__()
                self.bert = BertModel.from_pretrained("cointegrated/rubert-tiny2")
                for param in self.bert.parameters():
                    param.requires_grad = False
                self.linear = nn.Sequential(
                    nn.Linear(312, 256),
                    nn.Sigmoid(),
                    nn.Dropout(),
                    nn.Linear(256, 2)  # NUMBER_OF_CLASSES = 2
                )

            def forward(self, x):
                bert_out = self.bert(x[0], attention_mask=x[1])
                normed_bert_out = nn.functional.normalize(bert_out.last_hidden_state[:, 0, :])
                out = self.linear(normed_bert_out)
                return out

        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ BERT
        bert_model = MyTinyBERT()
        bert_model.eval()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
        state_dict = torch.load("bert4.pt", map_location=torch.device('cpu'), weights_only=True)
        bert_model.load_state_dict(state_dict)

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ BERT: {e}")

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è BERT
        tokenizer = BertTokenizer.from_pretrained("cointegrated/rubert-tiny2")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BERT: {e}")

    try:
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ LSTM
        class BahdanauAttention(nn.Module):
            def __init__(self, hidden_size: int):
                super().__init__()
                self.W_q = nn.Linear(hidden_size, hidden_size)
                self.W_k = nn.Linear(hidden_size, hidden_size)
                self.W_v = nn.Linear(hidden_size, 1)

            def forward(self, keys: torch.Tensor, query: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
                query = self.W_q(query)
                keys = self.W_k(keys)
                energy = self.W_v(F.tanh(query.unsqueeze(1) + keys)).squeeze(-1)
                att_weights = torch.softmax(energy, -1)
                context = torch.bmm(att_weights.unsqueeze(1), keys)
                return context, att_weights

        class LSTMBahdanauAttention(nn.Module):
            def __init__(self, vocab_size, embedding_dim, hidden_size):
                super().__init__()
                self.embedding = nn.Embedding(vocab_size, embedding_dim)
                self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)
                self.attn = BahdanauAttention(hidden_size)
                self.clf = nn.Sequential(
                    nn.Linear(hidden_size, 128),
                    nn.Dropout(0.2),
                    nn.Tanh(),
                    nn.Linear(128, 1),
                    nn.Dropout(0.1)
                )

            def forward(self, x):
                embeddings = self.embedding(x)
                outputs, (h_n, _) = self.lstm(embeddings)
                context, att_weights = self.attn(outputs, h_n.squeeze(0))
                out = self.clf(context)
                return out, att_weights  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–≤–∞ –∑–Ω–∞—á–µ–Ω–∏—è

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è LSTM
        word_to_int = joblib.load("word_to_int.joblib")

        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏ LSTM
        VOCAB_SIZE = len(word_to_int) + 1  # –£–∫–∞–∂–∏—Ç–µ —Ä–µ–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è
        EMBEDDING_DIM = 32
        HIDDEN_SIZE = 32
        lstm_model = LSTMBahdanauAttention(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE)
        lstm_model.load_state_dict(torch.load("lstm_model_weights.pt", map_location=torch.device('cpu')))
        lstm_model.eval()

    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ LSTM: {e}")

    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è LSTM
        word_to_int = joblib.load("word_to_int.joblib")
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–ª–æ–≤–∞—Ä—è word_to_int: {e}")

    return bow_pipeline, tfidf_pipeline, bert_model, lstm_model, tokenizer, word_to_int


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
def clean_text(text):
    morph = MorphAnalyzer()
    stop_words = set(stopwords.words('russian'))

    if not isinstance(text, str):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∞
        return ""

    text = text.lower()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    text = text.translate(str.maketrans('', '', string.punctuation))  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    tokens = text.split()  # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    tokens = [morph.parse(word)[0].normal_form for word in tokens if word not in stop_words]  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    tokens = [word for word in tokens if len(word) > 1]  # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    cleaned_text = " ".join(tokens).strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    return cleaned_text if cleaned_text else None


# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π
def predict_sentiment(text, bow_pipeline, tfidf_pipeline, bert_model, lstm_model, tokenizer, word_to_int):
    predictions = {}

    if bow_pipeline is not None and tfidf_pipeline is not None:
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        cleaned_text = clean_text(text)

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Bag-of-Words + Logistic Regression
        bow_prediction = bow_pipeline.predict([cleaned_text])[0]
        predictions["Bag-of-Words"] = "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π" if bow_prediction == 1 else "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π"

        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é TF-IDF + Logistic Regression
        tfidf_prediction = tfidf_pipeline.predict([cleaned_text])[0]
        predictions["TF-IDF"] = "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π" if tfidf_prediction == 1 else "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π"

    if bert_model is not None and tokenizer is not None:
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è BERT
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        with torch.no_grad():
            outputs = bert_model((inputs['input_ids'], inputs['attention_mask']))
        logits = outputs
        probs = torch.softmax(logits, dim=1)
        bert_prediction = torch.argmax(probs, dim=1).item()
        predictions["BERT"] = "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π" if bert_prediction == 1 else "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π"

    if lstm_model is not None and word_to_int is not None:
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è LSTM
        tokens = clean_text(text).split()
        sequence = [word_to_int.get(token, 0) for token in tokens]  # 0 - –∏–Ω–¥–µ–∫—Å –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Å–ª–æ–≤
        sequence = padding([sequence], seq_len=64)[0]  # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞–¥–¥–∏–Ω–≥
        sequence = torch.tensor(sequence).unsqueeze(0)  # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–∞—Ç—á–∞
        with torch.no_grad():
            lstm_output, _ = lstm_model(sequence)  # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥ –∏ –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        lstm_prediction = torch.sigmoid(lstm_output).squeeze().item() > 0.5
        predictions["LSTM"] = "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π" if lstm_prediction else "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π"

    return predictions
   


# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit
def main():
    st.title("–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤")
    st.write("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –µ–≥–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.")

    # –ü–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
    user_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:", "")

    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞
    if st.button("–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"):
        if user_input.strip() == "":
            st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞.")
        else:
            try:
                # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ —Å–ª–æ–≤–∞—Ä—è
                bow_pipeline, tfidf_pipeline, bert_model, lstm_model, tokenizer, word_to_int = load_models()

                # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                predictions = predict_sentiment(
                    text=user_input,
                    bow_pipeline=bow_pipeline,
                    tfidf_pipeline=tfidf_pipeline,
                    bert_model=bert_model,
                    lstm_model=lstm_model,
                    tokenizer=tokenizer,
                    word_to_int=word_to_int
                )

                # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
                for model_name, sentiment in predictions.items():
                    st.write(f"{model_name}: {sentiment}")

            except Exception as e:
                st.error(f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()




# import streamlit as st
# import torch
# import joblib
# from sklearn.pipeline import Pipeline
# from pymorphy3 import MorphAnalyzer
# from nltk.corpus import stopwords
# import string
# import torch.nn as nn
# from transformers import BertModel
# from streamlit_extras.let_it_rain import rain


# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∏–º–∞—Ü–∏–∏ –¥–æ–∂–¥—è
# def eye():
#     rain(
#         emoji="üëÅ",
#         font_size=100,
#         falling_speed=6,
#         animation_length="infinite",
#     )

# eye()


# # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
# st.image("savelyi.gif", width=1000)

# # HTML –∏ CSS –¥–ª—è –∞–Ω–∏–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
# html_code = """
# <style>
# @keyframes rainbow {
#     0% { color: red; }
#     14% { color: orange; }
#     28% { color: yellow; }
#     42% { color: green; }
#     57% { color: blue; }
#     71% { color: indigo; }
#     85% { color: violet; }
#     100% { color: red; }
# }

# .rainbow-text {
#     font-size: 3em;
#     font-weight: bold;
#     text-align: center;
#     animation: rainbow 5s infinite;
# }
# </style>

# <div class="rainbow-text">–í–°–ï –û–¢–ó–´–í–´ –ü–û –°–í–û–ï–ú–£ –ü–û–ó–ò–¢–ò–í–ù–´–ï!</div>
# """

# # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ HTML –≤ Streamlit
# st.markdown(html_code, unsafe_allow_html=True)


# # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è Bahdanau
# class BahdanauAttention(nn.Module):
#     def __init__(self, hidden_size):
#         super(BahdanauAttention, self).__init__()
#         self.attention = nn.Linear(hidden_size, 1)

#     def forward(self, lstm_out, h_n):
#         attention_weights = torch.softmax(self.attention(lstm_out), dim=1)
#         context_vector = torch.sum(attention_weights * lstm_out, dim=1)
#         return context_vector, attention_weights


# # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è LSTM —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è
# class LSTMBahdanauAttention(nn.Module):
#     def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):
#         super(LSTMBahdanauAttention, self).__init__()
#         self.embedding = nn.Embedding(vocab_size, embedding_dim)
#         self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)
#         self.attn = BahdanauAttention(hidden_size)
#         self.clf = nn.Sequential(
#             nn.Linear(hidden_size, 128),
#             nn.Dropout(0.2),
#             nn.Tanh(),
#             nn.Linear(128, num_classes),
#             nn.Dropout(0.1)
#         )

#     def forward(self, x):
#         embeddings = self.embedding(x)
#         outputs, (h_n, _) = self.lstm(embeddings)
#         context, att_weights = self.attn(outputs, h_n.squeeze(0))
#         out = self.clf(context)
#         return out, att_weights


# # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
# @st.cache_resource
# def load_models():
#     bow_pipeline = None
#     tfidf_pipeline = None

#     try:
#         # –ó–∞–≥—Ä—É–∑–∫–∞ pipeline –¥–ª—è Bag-of-Words + Logistic Regression
#         bow_pipeline = joblib.load("bow_pipeline.joblib")
#     except Exception as e:
#         st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ Bag-of-Words: {e}")

#     try:
#         # –ó–∞–≥—Ä—É–∑–∫–∞ pipeline –¥–ª—è TF-IDF + Logistic Regression
#         tfidf_pipeline = joblib.load("tfidf_pipeline.joblib")
#     except Exception as e:
#         st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ TF-IDF: {e}")

#     return bow_pipeline, tfidf_pipeline


# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
# def clean_text(text):
#     morph = MorphAnalyzer()
#     stop_words = set(stopwords.words('russian'))

#     if not isinstance(text, str):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å—Ç—Ä–æ–∫–∞
#         return ""

#     text = text.lower()  # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
#     text = text.translate(str.maketrans('', '', string.punctuation))  # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
#     tokens = text.split()  # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
#     tokens = [morph.parse(word)[0].normal_form for word in tokens if word not in stop_words]  # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
#     tokens = [word for word in tokens if len(word) > 1]  # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
#     cleaned_text = " ".join(tokens).strip()  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
#     return cleaned_text if cleaned_text else None


# # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π
# def predict_sentiment(text, bow_pipeline, tfidf_pipeline):
#     if bow_pipeline is None or tfidf_pipeline is None:
#         st.error("–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π.")
#         return {}

#     # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
#     cleaned_text = clean_text(text)

#     # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Bag-of-Words + Logistic Regression
#     bow_prediction = bow_pipeline.predict([cleaned_text])[0]

#     # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é TF-IDF + Logistic Regression
#     tfidf_prediction = tfidf_pipeline.predict([cleaned_text])[0]

#     # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
#     return {
#         "Bag-of-Words": "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π" if bow_prediction == 1 else "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π",
#         "TF-IDF": "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–π" if tfidf_prediction == 1 else "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π"
#     }


# # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Streamlit
# def main():
#     st.title("–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –æ—Ç–∑—ã–≤–æ–≤")
#     st.write("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞, —á—Ç–æ–±—ã –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –µ–≥–æ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å.")

#     # –ü–æ–ª–µ –¥–ª—è –≤–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞
#     user_input = st.text_area("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞:", "")

#     # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ç–µ–∫—Å—Ç–∞
#     if st.button("–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å"):
#         if user_input.strip() == "":
#             st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞.")
#         else:
#             # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
#             bow_pipeline, tfidf_pipeline = load_models()

#             # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
#             predictions = predict_sentiment(user_input, bow_pipeline, tfidf_pipeline)

#             # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
#             st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
#             for model_name, sentiment in predictions.items():
#                 st.write(f"{model_name}: {sentiment}")


# if __name__ == "__main__":
#     main()


